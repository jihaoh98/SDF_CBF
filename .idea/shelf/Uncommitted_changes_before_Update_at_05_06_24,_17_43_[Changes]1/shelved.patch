Index: point_mass/cdf.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nimport os\nimport sys\nimport torch\nimport matplotlib.pyplot as plt\nfrom robot2D_torch import Robot2D\nfrom primitives2D_torch import Box, Circle, Triangle, Ellipse, Union, Erode\nimport math\nimport matplotlib.animation as animation\n\nplt.rcParams['figure.dpi'] = 100\nPI = math.pi\nCUR_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\nclass CDF2D:\n    def __init__(self, device) -> None:\n        # super().__init__()\n        self.device = device\n        self.num_joints = 2\n        self.nbData = 50\n        self.Q_sets = self.create_Qsets(self.nbData).to(device)\n        self.link_length = torch.tensor([[2, 2]]).float().to(device)\n        self.obj_center_list = None\n\n        self.obj_lists = []\n        # one obstacle case\n        # self.obj_lists = [Circle(center=torch.tensor([2.0, -2.35]), radius=0.3, device=device)]\n\n        # # # two obstacles case\n        # self.obj_lists = [Circle(center=torch.tensor([2.3, -2.3]), radius=0.3, device=device),\n        #                   Circle(center=torch.tensor([0.0, 2.45]), radius=0.3, device=device, label='obstacle'), ]\n\n        # three obstacles case\n        # self.obj_lists = [Circle(center=torch.tensor([2.3, -2.3]), radius=0.3, device=device),\n        #                     Circle(center=torch.tensor([0.0, 2.45]), radius=0.3, device=device),\n        #                     Circle(center=torch.tensor([2.5, 1.5]), radius=0.3, device=device, label='obstacle'),]\n\n        # # one Box obstacle case\n        # self.obj_lists = [Box(center=torch.tensor([2.0, -2.35]), w=0.3, h=0.3, device=device, label='obstacle')]\n\n        self.q_max = torch.tensor([PI, PI]).to(device)\n        self.q_min = torch.tensor([-PI, -PI]).to(device)\n\n        # data generation\n        self.workspace = [[-3.0, -3.0], [3.0, 3.0]]\n        self.batchsize = 20000  # batch size of q\n        self.epsilon = 1e-3  # distance threshold to filter data\n\n        # robot\n        self.robot = Robot2D(num_joints=self.num_joints, init_states=self.Q_sets, link_length=self.link_length,\n                             device=device)\n\n        # # c space distance field\n        self.q_template = torch.load(os.path.join(CUR_PATH, 'data2D.pt'))\n\n    def add_object(self, obj):\n        self.obj_lists.append(obj)\n\n    def create_grid(self, nb_data):\n        t = np.linspace(-math.pi, math.pi, nb_data)\n        self.q0, self.q1 = np.meshgrid(t, t)\n        return self.q0, self.q1\n\n    def create_Qsets(self, nb_data):\n        q0, q1 = self.create_grid(nb_data)\n        q0_torch = torch.from_numpy(q0).float()\n        q1_torch = torch.from_numpy(q1).float()\n        Q_sets = torch.cat([q0_torch.unsqueeze(-1), q1_torch.unsqueeze(-1)], dim=-1).view(-1, 2)\n        return Q_sets\n\n    def inference_sdf(self, q):\n        # using predefined object\n        q.requires_grad = True\n        kpts = self.robot.surface_points_sampler(q)\n        B, N = kpts.size(0), kpts.size(1)\n        dist = torch.cat([obj.signed_distance(kpts.reshape(-1, 2)).reshape(B, N, -1) for obj in self.obj_lists], dim=-1)\n        # using the closest point from robot surface\n        sdf = torch.min(dist, dim=-1)[0]\n        sdf = sdf.min(dim=-1)[0]\n        grad = torch.autograd.grad(sdf, q, torch.ones_like(sdf), create_graph=True)[0]\n        return sdf, grad\n\n    def c_space_distance(self, q):\n        # x : (Nx,3)\n        # q : (Np,7)\n        # return d : (Np) distance between q and x in C space. d = min_{q*}{L2(q-q*)}. sdf(x,q*)=0\n\n        # compute d\n        Np = q.shape[0]\n        dist = torch.norm(q.unsqueeze(1) - self.q_template.unsqueeze(0), dim=-1)\n        d = torch.min(dist, dim=-1)[0]\n\n        # compute sign of d\n        d_ts = self.inference_sdf(q)\n        mask = (d_ts < 0)\n        d[mask] = -d[mask]\n        return d\n\n    def x_to_grid(self, p):\n        # p: (N,2)\n        # return grid index (N,2)\n        x_workspace = torch.tensor([self.workspace[0][0], self.workspace[1][0]]).to(self.device)\n        y_workspace = torch.tensor([self.workspace[0][1], self.workspace[1][1]]).to(self.device)\n\n        x_grid = (p[:, 0] - x_workspace[0]) / (x_workspace[1] - x_workspace[0]) * self.nbData\n        y_grid = (p[:, 1] - y_workspace[0]) / (y_workspace[1] - y_workspace[0]) * self.nbData\n\n        x_grid.clamp_(0, self.nbData - 1)\n        y_grid.clamp_(0, self.nbData - 1)\n        return torch.stack([x_grid, y_grid], dim=-1).long()\n\n    def inference_c_space_sdf_using_data(self, q):\n        # q : (N,2)\n        q.requires_grad = True\n        obj_points = torch.cat([obj.sample_surface(200) for obj in self.obj_lists])\n        grid = self.x_to_grid(obj_points)\n\n        q_list = (self.q_template[grid[:, 0], grid[:, 1]]).reshape(-1, 2)\n        q_list = q_list[q_list[:, 0] != torch.inf]\n        dist = torch.norm(q.unsqueeze(1) - q_list.unsqueeze(0), dim=-1)\n        d = torch.min(dist, dim=-1)[0]\n        grad = torch.autograd.grad(d, q, torch.ones_like(d), retain_graph=True)[0]\n        return d, grad\n\n    def plot_sdf(self):\n        sdf, grad = self.inference_sdf(self.Q_sets.requires_grad_(True))\n        sdf = sdf.detach().cpu().numpy()\n        cmap = plt.cm.get_cmap('coolwarm')\n        ct = plt.contourf(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData),\n                          cmap=cmap, levels=[-0.5, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], linewidths=1)\n        plt.clabel(ct, [], inline=False, fontsize=10, colors='black')\n        ct_zero = plt.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], linewidths=2,\n                              colors='k',\n                              alpha=1.0)\n        for c in ct_zero.collections:\n            c.set_hatch('///')  # Apply the hatch pattern\n        plt.xlabel('$q_0$', size=15)\n        plt.ylabel('$q_1$', size=15)\n\n    def plot_cdf(self, d):\n        sdf, grad = self.inference_sdf(self.Q_sets.requires_grad_(True))\n        sdf = sdf.detach().cpu().numpy()\n        cmap = plt.cm.get_cmap('coolwarm')\n        ct = plt.contourf(self.q0, self.q1, d.reshape(self.nbData, self.nbData),\n                          cmap=cmap, levels=[-0.5, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], linewidths=1)\n        plt.clabel(ct, [], inline=False, fontsize=10, colors='black')\n        ct_zero = plt.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], linewidths=2,\n                              colors='k',\n                              alpha=1.0)\n        for c in ct_zero.collections:\n            c.set_hatch('///')  # Apply the hatch patter\n        plt.xlabel('$q_0$', size=15)\n        plt.ylabel('$q_1$', size=15)\n\n    def plot_cdf_ax(self, d, ax):\n        sdf, grad = self.inference_sdf(self.Q_sets.requires_grad_(True))\n        sdf = sdf.detach().cpu().numpy()\n\n        # Make sure to use the axes object (ax) for plotting\n        contour = ax.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], linewidths=2,\n                             colors='k', alpha=1.0)\n        cmap = plt.cm.get_cmap('coolwarm')\n        contourf = ax.contourf(self.q0, self.q1, d.reshape(self.nbData, self.nbData),\n                               cmap=cmap, levels=[-0.5, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 6.0])\n        ax.clabel(contourf, [], inline=False, fontsize=10)\n\n        ct_zero = ax.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], colors='k')\n        for c in ct_zero.collections:\n            c.set_hatch('///')\n\n        ax.set_xlabel('$q_0$', size=15)\n        ax.set_ylabel('$q_1$', size=15)\n\n        return contour, contourf, ct_zero  # Ensure these match the names used in the animation function\n\n    def plot_sdf_ax(self, ax):\n        sdf, grad = self.inference_sdf(self.Q_sets.requires_grad_(True))\n        sdf = sdf.detach().cpu().numpy()\n        cmap = plt.cm.get_cmap('coolwarm')\n\n        # Make sure to use the axes object (ax) for plotting\n        contour = ax.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], linewidths=2,\n                             colors='k', alpha=1.0)\n        contourf = ax.contourf(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData),\n                               cmap=cmap, levels=[-0.5, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 6.0])\n        ax.clabel(contourf, [], inline=False, fontsize=10)\n\n        ct_zero = ax.contour(self.q0, self.q1, sdf.reshape(self.nbData, self.nbData), levels=[0], colors='k')\n        for c in ct_zero.collections:\n            c.set_hatch('///')\n\n        ax.set_xlabel('$q_0$', size=15)\n        ax.set_ylabel('$q_1$', size=15)\n\n        return contour, contourf, ct_zero  # Ensure these match the names used in the animation function\n\n    def plot_robot(self, q, color='black'):\n        # plot robot\n        for _q in q:\n            f_rob = self.robot.forward_kinematics_all_joints(_q.unsqueeze(0))[0].detach().cpu().numpy()\n            plt.plot(f_rob[0, :], f_rob[1, :], color=color)  # plot link\n            plt.plot(f_rob[0, 0], f_rob[1, 0], '.', color='black', markersize=10)  # plot joint\n\n    def plot_objects(self):\n        for obj in self.obj_lists:\n            plt.gca().add_patch(obj.create_patch())\n\n    def plot_robot_trajectory(self, points, ax):\n        # plot obstacles\n\n        for obj in self.obj_lists:\n            ax.add_patch(obj.create_patch())\n        # plot robot\n        k = 10  # Replace 5 with whatever step size you want\n        for i in range(0, len(points), k):\n            _q = points[i]\n            f_rob = self.robot.forward_kinematics_all_joints(_q.unsqueeze(0))[0].detach().cpu().numpy()\n            plt.plot(f_rob[0, :], f_rob[1, :], color='black')\n            plt.pause(0.001)\n\n    def anima_robot_trajectory(self, geodesic):\n        # Animation setup\n        t = torch.linspace(0.0, 1.0, 100).to(self.device)\n        curve_points = geodesic(t)\n        fig, ax = plt.subplots()\n\n        # Initialize trajectory line\n        line, = ax.plot([], [], color='black')\n\n        # Function to update the line in each frame\n        def update(frame):\n            _q = curve_points[frame]\n            f_rob = self.robot.forward_kinematics_all_joints(_q.unsqueeze(0))[0].detach().cpu().numpy()\n            line.set_data(f_rob[0, :], f_rob[1, :])\n            return line,\n\n        # # Plotting obstacles\n        # for obj in self.obj_lists:\n        #     ax.add_patch(obj.create_patch())\n\n        ani = animation.FuncAnimation(fig, update, frames=len(curve_points), blit=True)\n\n        # Show or save animation\n        plt.show()\n        # ani.save('animation.mp4')  # Uncomment to save the animation\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    cdf = CDF2D(device)\n    # d, grad = cdf.inference_c_space_sdf_using_data(cdf.Q_sets)\n    # cdf.plot_c_space_distance(d.detach().cpu().numpy())\n    # cdf.plot_sdf()\n\n    # q = torch.tensor([0.0, torch.deg2rad(torch.tensor(45))]).view(1, 2).to(device)\n    # cdf.plot_sdf()\n    # d = cdf.c_space_distance(q)\n    # cdf.plot_c_space_distance(d.detach().cpu().numpy())\n    # cdf.plot_objects()\n    # cdf.plot_robot(q)\n    # plt.show()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/point_mass/cdf.py b/point_mass/cdf.py
--- a/point_mass/cdf.py	
+++ b/point_mass/cdf.py	
@@ -18,7 +18,7 @@
         # super().__init__()
         self.device = device
         self.num_joints = 2
-        self.nbData = 50
+        self.nbData = 100
         self.Q_sets = self.create_Qsets(self.nbData).to(device)
         self.link_length = torch.tensor([[2, 2]]).float().to(device)
         self.obj_center_list = None
@@ -52,7 +52,7 @@
                              device=device)
 
         # # c space distance field
-        self.q_template = torch.load(os.path.join(CUR_PATH, 'data2D.pt'))
+        self.q_template = torch.load(os.path.join(CUR_PATH, 'data2D_100.pt'))
 
     def add_object(self, obj):
         self.obj_lists.append(obj)
